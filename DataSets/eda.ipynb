{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.1.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/thytran/Documents/GitHub/progresses/Synthetic-AI/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached pillow-11.1.0-cp39-cp39-macosx_11_0_arm64.whl (3.1 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, pyparsing, pillow, kiwisolver, joblib, importlib-resources, fonttools, cycler, contourpy, click, nltk, matplotlib, seaborn\n",
      "Successfully installed click-8.1.8 contourpy-1.3.0 cycler-0.12.1 fonttools-4.57.0 importlib-resources-6.5.2 joblib-1.4.2 kiwisolver-1.4.7 matplotlib-3.9.4 nltk-3.9.1 pillow-11.1.0 pyparsing-3.2.3 regex-2024.11.6 seaborn-0.13.2 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas matplotlib seaborn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"raw/AI_Human.csv\")  # Replace \"your_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Inspection\n",
    "##### 1. Shape and Size\n",
    "##### 2. Missing Values\n",
    "##### 3. First Few Rows\n",
    "##### 4. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(487235, 2)\n",
      "text          object\n",
      "generated    float64\n",
      "dtype: object\n",
      "text         0\n",
      "generated    0\n",
      "dtype: int64\n",
      "                                                text  generated\n",
      "0  Cars. Cars have been around since they became ...        0.0\n",
      "1  Transportation is a large necessity in most co...        0.0\n",
      "2  \"America's love affair with it's vehicles seem...        0.0\n",
      "3  How often do you ride in a car? Do you drive a...        0.0\n",
      "4  Cars are a wonderful thing. They are perhaps o...        0.0\n",
      "           generated\n",
      "count  487235.000000\n",
      "mean        0.372383\n",
      "std         0.483440\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.shape) # Shape and Size\n",
    "print(df.dtypes) # Data Types of each column\n",
    "print(df.isnull().sum())  # Missing values per column\n",
    "print(df.head()) # First few rows\n",
    "print(df.describe()) # Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-specific EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Text Statistics:\n",
    "##### 1. Document Length: Calculate the length of each text (number of characters, number of words, number of sentences).\n",
    "##### 2. Average Word Length: Calculate the average length of words in each text.\n",
    "##### 3. Vocabulary Size: Determine the number of unique words in each text and in the entire dataset.\n",
    "##### 4. Sentence Length: Calculate the number of words per sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          char_count     word_count  avg_word_length\n",
      "count  487235.000000  487235.000000    487235.000000\n",
      "mean     2269.586592     393.096214         4.755985\n",
      "std       988.814028     168.593328         0.521039\n",
      "min         1.000000       0.000000         0.000000\n",
      "25%      1583.000000     278.000000         4.415595\n",
      "50%      2102.000000     363.000000         4.685144\n",
      "75%      2724.000000     471.000000         5.020000\n",
      "max     18322.000000    1668.000000       126.000000\n",
      "Vocabulary size: 544828\n"
     ]
    }
   ],
   "source": [
    "df['char_count'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "#df['sentence_count'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "\n",
    "def avg_word_len(text):\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words) if len(words) > 0 else 0\n",
    "\n",
    "df['avg_word_length'] = df['text'].apply(avg_word_len)\n",
    "\n",
    "print(df[['char_count', 'word_count', 'avg_word_length']].describe())\n",
    "\n",
    "# Vocabulary size (number of unique words)\n",
    "all_words = ' '.join(df['text']).split()\n",
    "unique_words = set(all_words)\n",
    "print(f\"Vocabulary size: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Splitting Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set the desired ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired ratios\n",
    "train_ratio = 1000\n",
    "val_ratio = 100\n",
    "test_ratio = 100\n",
    "total_ratio = train_ratio + val_ratio + test_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired subset size (adjust as needed)\n",
    "subset_size = 10000\n",
    "\n",
    "# Calculate the number of subsets\n",
    "num_subsets = len(df) // subset_size  # Integer division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the sizes of each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sizes of each split\n",
    "train_size = int(train_ratio / total_ratio * len(df))\n",
    "val_size = int(val_ratio / total_ratio * len(df))\n",
    "test_size = len(df) - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data once before creating subsets\n",
    "data = df.sample(frac=1, random_state=42).reset_index(drop=True) # frac=1 means shuffle all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create the splits\n",
    "train_data = df.iloc[:train_size]\n",
    "val_data = df.iloc[train_size:train_size + val_size]\n",
    "test_data = df.iloc[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main directory\n",
    "main_dir = \"subsets\"\n",
    "os.makedirs(main_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 2:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 3:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 4:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 5:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 6:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 7:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 8:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 9:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 10:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 11:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 12:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 13:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 14:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 15:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 16:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 17:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 18:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 19:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 20:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 21:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 22:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 23:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 24:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 25:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 26:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 27:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 28:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 29:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 30:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 31:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 32:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 33:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 34:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 35:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 36:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 37:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 38:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 39:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 40:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 41:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 42:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 43:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 44:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 45:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 46:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 47:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Subset 48:\n",
      "  Train data size: 8333\n",
      "  Validation data size: 833\n",
      "  Test data size: 834\n",
      "Remaining Data Subset:\n",
      "  Train data size: 6029\n",
      "  Validation data size: 602\n",
      "  Test data size: 604\n"
     ]
    }
   ],
   "source": [
    "# Create the subsets\n",
    "for i in range(num_subsets):\n",
    "    # Create subfolder for each subset\n",
    "    subset_dir = os.path.join(main_dir, f\"subset_{i+1}\")\n",
    "    os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "    # Get the subset of the data\n",
    "    start_index = i * subset_size\n",
    "    end_index = (i + 1) * subset_size\n",
    "    subset = data.iloc[start_index:end_index]\n",
    "\n",
    "    # Calculate the sizes of each split within the subset\n",
    "    train_size = int(train_ratio / total_ratio * len(subset))\n",
    "    val_size = int(val_ratio / total_ratio * len(subset))\n",
    "    test_size = len(subset) - train_size - val_size\n",
    "    \n",
    "        # Split the subset into train, validation, and test sets\n",
    "    train_data = subset.iloc[:train_size]\n",
    "    val_data = subset.iloc[train_size:train_size + val_size]\n",
    "    test_data = subset.iloc[train_size + val_size:]\n",
    "\n",
    "    # Print the sizes of the resulting datasets for this subset\n",
    "    print(f\"Subset {i+1}:\")\n",
    "    print(f\"  Train data size: {len(train_data)}\")\n",
    "    print(f\"  Validation data size: {len(val_data)}\")\n",
    "    print(f\"  Test data size: {len(test_data)}\")\n",
    "\n",
    "    # Save the sub-datasets to CSV files in the subset directory\n",
    "    train_data.to_csv(os.path.join(subset_dir, 'train.csv'), index=False)\n",
    "    val_data.to_csv(os.path.join(subset_dir, 'validation.csv'), index=False)\n",
    "    test_data.to_csv(os.path.join(subset_dir, 'test.csv'), index=False)\n",
    "    \n",
    "# Handle any remaining data (less than subset_size)\n",
    "if len(data) % subset_size != 0:\n",
    "    # Create subfolder for remaining data\n",
    "    remaining_dir = os.path.join(main_dir, \"subset_remaining\")\n",
    "    os.makedirs(remaining_dir, exist_ok=True)\n",
    "\n",
    "    start_index = num_subsets * subset_size\n",
    "    remaining_data = data.iloc[start_index:]\n",
    "\n",
    "    train_size = int(train_ratio / total_ratio * len(remaining_data))\n",
    "    val_size = int(val_ratio / total_ratio * len(remaining_data))\n",
    "    test_size = len(remaining_data) - train_size - val_size\n",
    "\n",
    "    train_data = remaining_data.iloc[:train_size]\n",
    "    val_data = remaining_data.iloc[train_size:train_size + val_size]\n",
    "    test_data = remaining_data.iloc[train_size + val_size:]\n",
    "\n",
    "    print(\"Remaining Data Subset:\")\n",
    "    print(f\"  Train data size: {len(train_data)}\")\n",
    "    print(f\"  Validation data size: {len(val_data)}\")\n",
    "    print(f\"  Test data size: {len(test_data)}\")\n",
    "\n",
    "    train_data.to_csv(os.path.join(remaining_dir, 'train.csv'), index=False)\n",
    "    val_data.to_csv(os.path.join(remaining_dir, 'validation.csv'), index=False)\n",
    "    test_data.to_csv(os.path.join(remaining_dir, 'test.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
